---
title: "Demo #5"
subtitle: "Cross Validation"
author: "Jeffrey Yarus"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 6
  word_document: 
    toc: yes
    toc_depth: '6'
  html_document: default
  always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

# Demo Summary

In this demo, the objective is to cross validate the variogram model we created previously. 

**Please be sure to read through the text I have included before each code chunk.**

If you wish to run the demo, be sure to **run the entire code chunk by chunk.** The variable P_PHI_pct is already in place and ready to run. The variogram model and kriging parameters are already set. **Erase your Global Environment before beginning**
  
**We will recreate the experimental semivariogram and variogram model from the earlier demo using ther variable, P_PHI_pct**

  - Be sure you are using the appropriate database with NO outlier!
  
  - The objective is to evaluate the performance of your variogram model on the kriging or conditional simulation results and provide a method for selecting the "best" variogram model.

  - Run the cross validation code chunks to help us determine which of several variogram model(s) work best (i.e., which generated the fewest number of over and under estimates). The idea is to run cross validation on the variogram models you think are best. Then, see which one generates the fewest over and under estimates.

**Be careful not to overfit the variogram models.** 

  - Keep in mind that selecting a bunch of variogram models and allowing the automatic fitting function from *gstlearn* to generate a really nice fit to the experimental variogram, does not mean that the resulting kriged map will be reasonable! There is a risk of overfitting.  As a rule of thumb, when using automatic fitting procedure, selecting fewer model types to evaluate is generally better than selecting many.

**Reproduce the kriged map for isotropic Model** for P_PHI and its Error Variance (Standard Deviation) Maps using the "best" variogram model you constructed. Be sure you update (change) the various titles, axes names, input names, as appropriate within the body of each chunk where appropriate. 
  
# Terminology and key functions from this Demo

1. **Cross Validation** or "leave one out"
  A bootstrapping method that will assess the value of each known sample one at a time and compare the estimated value with the actual value. The estimated value should be within 2.5 standard deviations of the known value. At the end of the analysis, a number of graphical statistics are presented. The preferred variogram model is the one with the least number of over or under estimates 
  
2. **Estimation Error**
  The estimation error is the difference between the actual value and the estimated value

3. **Standardized Error**
  The standardized error is the normalized error in units of standard deviation

# Loading Packages

```{r include = FALSE}
library(corrplot)
library(PerformanceAnalytics)
library(tidyverse)
library(fastDummies)
library(magrittr)
library(GGally)
library(ggplot2)
library(RColorBrewer)
library(knitr)
library(ggpubr)
library(gstlearn)
library(gridGraphics)
library(here)
```

Next chunk tells you where you are.

```{r}
getwd()
```

# Initializing the Demo

As in the previous demos, we:

  - Read in our data
  - Create the data frame (tibble)
  - Ensure our categorical variables (if present) are designated as factors
  - Create a backup data frame
  - Remove the unnecessary variables from the tibble
  - Create our assigned symbolic names
  - Identify the outliers.
  
  Run the chunk below... BE SURE TO SET THE PATH CORRECTLY!
```{r }
file.name <- "../../data/WT-2D-all-outlier.csv" 
df <- read_csv(file.name)
df %<>% 
   mutate(across(matches("N_|L_"), factor))

# Creating a duplicate data frame for later use, but eliminate unnecessary variables

df2 <- df %>%  
  dplyr::select(L_3_FACIES:P_Top_ft)

# Creating Symbolic Inputs

xlon <- "C_X_ft"
ylat <- "C_Y_ft"
property <- "P_PHI_pct"
head(df, n = 20)

# Identifying the outliers

df <-
  df %>%
  mutate(
    iqr_val = IQR(!!sym(property)),
    iqr_val_adj = ((iqr_val) * 1.5),
    third_q = quantile(!!sym(property), prob = 0.75, na.rm = TRUE),
    first_q = quantile(!!sym(property), prob = .25, na.rm = TRUE),
    outlier =
      (!!sym(property)) > (third_q + iqr_val_adj) |
      (!!sym(property) < (first_q - iqr_val_adj))) %>%
    mutate(Log_property = log(!!sym(property)))
```

#  Preparation for Geostatistical Analysis using gstlearn

## Database

The following chunk creates a database **that does not include outliers**

## Creating databases with no outliers

Here, we "filter out" the outliers (when they are present)

```{r warning = FALSE}
# Creating databases with no outliers (removing the first variable)
df1 = df %>% dplyr::select(-F_Top) %>% filter(outlier == FALSE)

# creating a database from the Data Frame
db_noout.db = Db_fromTL(df1)

# Define the different variables
err = db_noout.db$setLocators(c(xlon, ylat), ELoc_X())
err = db_noout.db$setLocator(property, ELoc_Z())
db_noout.db 
```

## Plotting a basemap from a database

We will plot the base-map to ensure we have identified and removed outliers

```{r, warning=FALSE}
p = plot.init(asp=1)
p = p + plot.symbol(db_noout.db, nameColor = property, pch = 19, cex = 1, flagLegend = TRUE)
p = p + plot.decoration(title=paste("Basemap of", property, "with No Outlier"), xlab = "X (UTM)", ylab = "Y (UTM)")
plot.end(p)
```

The base-map looks good, so we will continue.

## Construction of the Experimental Variogram

### Omnidirectional Variogram 

Be sure to look at the information printed out on the omnidirectional variogram 
The code used here is similar to what would be used for directional variograms

Run the chunk below...

```{r, warning = FALSE}
varioparam <- VarioParam_createOmniDirection(nlag = 10, dlag=2000)
vario_omni <- Vario_computeFromDb(varioparam, db_noout.db)
```

### Visualizing the Variogram

```{r, warning = FALSE}
p = plot.init()
p = p + plot.vario(vario_omni, drawPlabel = TRUE)
p = p + plot.decoration(title = paste(property, "Experimental Ominidirectional Variogram"), 
                        xlab = "Lag distance", 
                        ylab = expression(paste("Variance (", gamma, "(h))", sep="")))
plot.end(p)
```

### Select basic structures for fitting this experimental variogram

Here I have selected a the Exponential basic structure for the demo.  However, that may not be the correct variogram model.  You will need to select from the variogram list what you believe to be the "best" variogram model that you will then validate through cross validation. 

Make your variogram selection in this code chunk. It's possible to include more than 1 basic structure to fit the model with, the program will calculate and pick out the one that fits best. In the chunk below, I have specified only one basic structure.  If you specify more than one, the fitting algorithm will decide which combination is best.  Be careful, the algorithm will be inclined to build nested structures to precisely fit the experimental variogram, so overfitting is very possible!  Less is generally better. **Also note that the specification of the basic structure(s) can be done by explicitly naming the ones you want from the list below**

```{r}
ECov_printAll()
```

Select the basic structure(s) you wish to use

```{r}
types = ECov_fromKeys(c("NUGGET", "EXPONENTIAL", "SPHERICAL", "CUBIC"))
```

## Fitting the Variogram Model

### Isotropic Variogram Model 

Having only calculated an omnidirectional variogram, it makes sense to fit it with a Model which is similar whatever the direction: an **isotropic** Model.

```{r, warning = FALSE}
data.model.omni <- Model()
err = data.model.omni$fit(vario_omni, types=types)
data.model.omni
```

In the next chunk, we visualize the Model together with the experimental variogram to appreciate the quality of the fit: the algorithm tries to minimize the (weighted) sum of the squared distance between the value of each variogram lag and the Model at the corresponding distance. The weight accounts for the distance (higher weight for smaller distance) and the number of pairs per lag (higher weight for larger number of pairs).

```{r, warning = FALSE}
p = plot.init() 
p = p + plot.varmod(vario_omni, data.model.omni, flagLegend = TRUE)
p = p + plot.decoration(title = paste(property, "Model Omnidirectional"), xlab = "Lag distance",
                        ylab = expression(paste("Variance (", gamma, "(h))", sep = "")))
plot.end(p)
```

# Neighborhood design

When performing cross validation, a neighborhood must be defined. Recall, the neighborhood defines the region within which samples are selected for use in the kriging estimation algorithm.

The choice of neighborhood  and the neighborhood design is important, particularly when there is a lot of data.  The basic choices are either a **Unique** neighborhood or a **Moving** neighborhood.  A unique, or global neighborhood requires essentially no parameterization and uses all the sample data. This could be problematic if there were a lot of data as it would greatly slow the computational time. 

A moving neighborhood is more efficient when dealing with large data sets.  However, it requires more parameterization like; the size of the search circle or ellipse, the number of sectors that divide the neighborhood in to equal size compartments, the optimal number of samples to gather in each sector, and the optimal number of samples to gather overall.  

In this case, there are only 261 samples and this is considered quite small, so we will use the unique neighborhood option. 

```{r}
neigh.unique <- NeighUnique_create()
```

We can also create a moving neighborhood that we can use for comparison.  Recall that a moving neighborhood restricts the selected neighboring points to those closer to the estimation node.  In this case, selecting up to the closest 50 neighbors within 10,000m of the estimation node.   

```{r}
myneigh <- NeighMoving_create(nmaxi=50, radius=10000)
```

# Evaluation of the variogram using Cross Validation

The function **xvalid** (for cross-validation in gstlearn) produces estimates of the known sample data. It does this by dropping one sample value at a time and using the neighboring data along with the kriging algorithm to re-estimate it.  This way we can compare the actual values to the kriging estimates.  Recall that kriging requires a Model which we have modeled using the code above.  If we change the variogram model, the estimates will change.  We can construct different variogram models and run the cross validation exercise to help us determine which variogram model works best. 

The code produces a set of new variables that are listed in the database identified for the cross validation.  The new variables are as follows: 

  - estim:      Estimated value (Z*)
  - stdev:      Standard Deviation (S)
  - esterr:     Estimation Error (Z-Z*), the difference between the predicted value generated by a model and the true
                value of the target variable accounting for factors such as bias, variance, and irreducible error that
                may influence the model's performance on unseen data.
  - stderr:     Standardized Error (sometimes referred to as the standardized residual), the difference
                between an observed value and its expected value, divided by the standard deviation of the error term:
                (Z*-Z)/S).  Don't confuse this with, "Standard Error" which refers to the variability or dispersion of
                a sample statistic relative to the true population parameter.  

In this code chunk, we select the appropriate variable location. Note the specific arguments used in NamingConvention. The first argument gives the **prefix** which will be systematically added to the variable name and the result type. The fourth one (set to FALSE) disables the capacity of setting automatically the newly calculated variables as the data variable (locator Z): thus, we do not have to set the appropriate variable between the two calls to **xvalid**.

```{r warning = FALSE}
err = db_noout.db$clearLocators(ELoc_Z())
err = db_noout.db$setLocator(property, ELoc_Z())
err = xvalid(db = db_noout.db, 
    model = data.model.omni,
    neigh = neigh.unique,    # you can replace it by "myneigh'
    flag_xvalid_est = -1,    # If 1, (Z*-Z, esterr). If -1, (Z*, est)
    flag_xvalid_std = -1,    # If 1, (Z*-Z)/S. If -1, (S) S = Std Dev of est error
    namconv = NamingConvention("XvalUniq1", TRUE, TRUE, FALSE)  # Adapt for Moving Neighborhood
  )

err = xvalid(db = db_noout.db, 
    model = data.model.omni,
    neigh = neigh.unique,    
    flag_xvalid_est = 1,    # If 1, (Z*-Z, esterr). If -1, (Z*, est)
    flag_xvalid_std = 1,    # If 1, (Z*-Z)/S. If -1, (S) S = Std Dev of est error
    namconv = NamingConvention("XvalUniq1", TRUE, TRUE, FALSE)
  )
db_noout.db
```

**If you select a different variable to analyze**

You will need to change the name of the estimation variable. The code will generate the following variables: The estimate (estim), the standard deviation value (stdev), the estimation error value (esterr), and the standard error value (stderr) for the variable being analyzed.  

## Graphical results of Cross Validation

It is possible to display the various cross validation results.  However, sometimes it's easier to use S3 functions like those of ggplot2.  So, the code below converts the database (db_noout.db) into a data frame (df_db_noout).

```{r, message = FALSE, warning = FALSE}
df_db_noout <- db_noout.db$toTL()

P_property <- 
  df_db_noout %>%
  ggplot(aes
  (!!sym(property))) +
  geom_histogram(
    fill = "red", 
    color = "white",
    bins = 21) +
  xlim(0,15) + 
  labs(x = "Data")

P_estim <- 
  df_db_noout %>%
    ggplot(
      aes(
       XvalUniq1.P_PHI_pct.estim)) +  #change to Xvalid.P_KH_md.estim
  geom_histogram(
    fill = "red", 
    color = "white",
    bins = 21) +
  xlim(0,15) + 
  labs(x = "Estimation")

P_stdev <- 
  df_db_noout %>%
  ggplot(
    aes(
      XvalUniq1.P_PHI_pct.stdev)) + 
  geom_histogram(
    fill = "red", 
    color = "white",
    bins = 21) +
  labs(x = "Standard Deviation")

P_esterr <- 
  df_db_noout %>%
  ggplot(
    aes(
      XvalUniq1.P_PHI_pct.esterr)) +
  geom_histogram(
    fill = "red", 
    color = "white",
    bins = 21) +
    annotate("rect", xmin = -4.5, xmax = -2.5, ymin = 0, ymax = 4.8,
           alpha = .5,fill = "darkblue") +
    annotate("rect", xmin = 2.7, xmax = 5.0, ymin = 0, ymax = 4,
           alpha = .5,fill = "darkblue") +
  labs(x = "Estimation Error")

P_stderr <- 
  df_db_noout %>%
  ggplot(
    aes(
      XvalUniq1.P_PHI_pct.stderr)) + 
  geom_histogram(
    fill = "red", 
    color = "white",
    bins = 21) +
    annotate("rect", xmin = -4.5, xmax = -2.5, ymin = 0, ymax = 4.8,
           alpha = .5,fill = "darkblue") +
    annotate("rect", xmin = 2.7, xmax = 5.0, ymin = 0, ymax = 4,
           alpha = .5,fill = "darkblue") +
  labs(x = "Standardized Error")

P_cross <- 
df_db_noout %>%
  ggplot(
    aes(
      y = !!sym(property), 
      x = XvalUniq1.P_PHI_pct.estim)) + 
  geom_point(
    col = "blue") +
  geom_smooth(
    method = 'lm',
    col = "red",
    formula = y ~ x,
    se = FALSE
  ) +
  labs(y = "Data") + 
  labs(x = "Estimation") 

ggarrange(
  P_property + 
    theme(axis.title.y = element_blank()),
  P_estim + 
    theme(axis.title.y = element_blank()),
  P_stdev + 
    theme(axis.title.y = element_blank()),
  P_esterr + 
    theme(axis.title.y = element_blank()),
  P_stderr + 
    theme(axis.title.y = element_blank()),
  P_cross + 
    theme()
)
```

Using gstlearn functions, we can look more closely at a few statistical graphs to see the estimates that are beyond +/- 2.5 standard deviations from mean.The graphics are:

- the base-map where the samples are represented with a size proportional to the absolute value of the standardized error. Note that the Data Base is duplicated ao define a selection of the values where *stderr* is smaller than -2.5 (resp. larger than 2.5)

```{r}
db_low.db = db_noout.db$clone()
iuid = db_low.db$addSelectionByVariable("*.stderr", upper=-2.5, name="sel_low")
db_sup.db = db_noout.db$clone()
iuid = db_sup.db$addSelectionByVariable("*.stderr", lower=2.5, name="sel_sup")
```

```{r warning = FALSE}
p = plot.init(asp=1)
p = p + plot.symbol(db_noout.db, nameSize = "*.stderr", flagAbsSize=TRUE, pch = 21, color='black')
p = p + plot.symbol(db_low.db, nameSize = "*.stderr", flagCst=TRUE, pch = 19, color='red')
p = p + plot.symbol(db_sup.db, nameSize = "*.stderr", flagCst=TRUE, pch = 19, color='blue')
p = p + plot.decoration(title="Standardized Error (abs) - Threshold = 2.5")
plot.end(p)
```

- the cross plot of the standardized error against the estimated values, 

```{r warning = FALSE}
p = plot.init(asp=1)
p = p + plot.correlation(db_noout.db, namex = "*.estim", namey = "*stderr", asPoint = TRUE)
p = p + plot.correlation(db_low.db, namex = "*.estim", namey = "*stderr", asPoint = TRUE, color = 'red')
p = p + plot.correlation(db_sup.db, namex = "*.estim", namey = "*stderr", asPoint = TRUE, color = 'blue')
p = p + plot.decoration(title="(Z-Z*)/S* vs. Z*", xlab = "Estimation", ylab = "Standardized Error")
p = p + geom_hline(yintercept = 0, lty = 1, color="black")
p = p + geom_hline(yintercept = -2.5, lty = 1, color="red")
p = p + geom_hline(yintercept = 2.5, lty = 1, color="blue")
plot.end(p)
```

- the cross plot of the estimated values against the actual values 

```{r warning = FALSE}

p = plot.init(asp=1)
p = p + plot.correlation(db_noout.db, namex = "*.estim", namey = property, asPoint = TRUE, 
                         flagBiss=TRUE, bissColor='black')
p = p + plot.correlation(db_low.db, namex = "*.estim", namey = property, asPoint = TRUE, col = 'red')
p = p + plot.correlation(db_sup.db, namex = "*.estim", namey = property, asPoint = TRUE, col = 'blue')
p = p + plot.decoration(title="Z vs. Z*", xlab = "Estimation", ylab="Data")
plot.end(p)
```

Look at the various graphs just produced.  The red values are those data samples where the estimates are less than -2.5 standard deviations from mean and the blue ones are for those where the estimates is more than 2.5 standard deviations from the mean. 

From here, we can try different variogram model types to see which generated the fewest over and under estimates.  NOTE: Fewer over and under estimates is a very mechanical way of determining which variogram model is best. In practice, you should use your best judgement as a domain expert to make a final decision  
**End of Demo #5**
